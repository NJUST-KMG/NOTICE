每周日上午周期会议ID：578 6499 0634

2021.2.28（周日）上午10：00
张：1.关于paddle中CV与NLP库的介绍。2.跨模态检索数据集与SOTA等介绍
吴：

2021.2.21（周日）上午10:00
魏：论文：Auto-Encoding Scene Graphs for Image Captioning
鲍：论文：Recent Work

2021.2.07（周日）上午10:00
郭：论文：Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning
      Q1.文本层级图是有向和无向的？
         是无向的。
      Q2.视频检索和图像检索有什么区别？
         待回答
2021.1.30（周日）上午10:00
王：论文：Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspective

2021.1.24（周日）上午10:00
倪：论文：Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels

2021.1.17（周日）上午10:00
倪：论文：Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels

2021.1.10（周日）上午10:00
傅：论文：CONAN:Complementary Pattern Augmentation for Rare Disease Detection

2021.1.3（周日）晚19：00
傅：论文：CONAN:Complementary Pattern Augmentation for Rare Disease Detection

2020.12.27（周日）晚19:00
王：论文：Graph Structured Network for Image-Text Matching
      Q1.如何将极坐标边权矩阵转化为正常的边权矩阵？
         将极坐标输入到一组可学习均值和协方差的K Gaussian kernels，
         其中均值可解释为极坐标中的方向和距离，详细见Learning Conditioned Graph Structures for Interpretable Visual Question Answering

2020.12.20（周日）晚19:00
倪：论文：Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels

2020.12.13（周日）晚19：00
郭：论文：AUC Optimization with a Reject Option
       Q1.拒绝率是怎么算的？
              引用文献中的解释是这样的：
              Once the training process had been completed, we assessed the performances of the trained models
              with the test dataset. The a posteriori probabilities were obtained and used as the outcome. These
              a posteriori probabilities were then used to calculate accuracy as a function of rejection
              rate. The range of a posteriori probabilities (from 0 to 1) was subdivided into 500 rejection
              regions. This process was repeated 100 times to obtain the final mean ARCs.
              我的理解是：在测试集上估计每个样本的后验概率P(y=1|x)，并与设定的阈值α（可以将[0,α]看作是一个拒绝区间）比较，如果在这个区间就拒绝，反之接受，可以算出test上的拒绝率。
              因此，每个α都对应一个拒绝率，显然，α越大，拒绝率越高。同时α也对应一个准确率，因此可以plot出拒绝率-准确率曲线。
       Q2.为什么拒绝率在0.5表现比0上好？
              Chow(1957)引入了拒绝选项建议，对于后验概率不够高（指达不到阈值α）的样本不应进行分类，以减少错误的可能性。如果预测不够可靠，分类器会拒绝一个示例，并落入拒绝区域。
              错误率和拒绝率之间有一个一般的关系:根据Chow(1970)，错误率随着拒绝率的增加而单调递减。
       Q3.为什么用平方损失作为代替的凸损失函数？
              引用文献的解释：
              首先，平方损失的主要优点在于，它足以存储训练样本的一阶和二阶统计信息进行优化，导致内存需求为O(d2)，这与训练样本的数量无关。(这个地方我不是很明白)
              其次，平方损失函数和AUC值是一致的，一致的含义是，把loss函数换成该平方函数后经验风险收敛能推出带有0-1loss的经验风险收敛。该定理的证明见Gao,W., and Zhou, Z.-H. 2015. On the consistency of AUC pairwise optimization.

2020.12.06（周日）晚19：30
傅：论文：A survey on deep learning in medical image analysis

2020.11.29（周日）晚19:00
倪：论文：Learning to learn from noisy labeled data
       Q1.监督信息是什么？
             有监督与无监督值的是人类监督，具体指是否使用了人工处理过的数据，比如人工挑选一些图片作为种子图片用来清除噪声标签，
             或者通过人工标注来为混淆度打分，因此他们具有可扩展性上的缺陷，因为需要对每一类数据都标注，如噪声种类特别多的时候。         
       Q2.Joint Optimization是什么方法？
             Joint Optimization是一种将几种损失函数联合到一起来更新的一种方法，发表在CVPR2018上。

2020.11.22（周日）晚19：00
王：论文：A Survey on Knowledge Graphs Representation Acquisition and Applications
       Q1.对于SimplE中提出的enbedding，实体和关系是如何嵌入的？
             在此模型中，将每个关系r对应一个向量r，每个实体e对应两个向量h(e)和t(e)，
             在进行embedding时，当e做头结点时用h(e)，做尾节点时用t(e),且嵌入的过程h(e)和t(e)是独立的，互不影响。
             而关系就是做逆。
郭：论文：Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
       Q1.数据不平衡的分类：step imbalance和long-tailed imbalance的区别？
             long-tailed imbalance指的是类的数据数量呈现长尾分布，即不断降低的一条曲线。
             step imbalance指的是频繁类和少数类的数据量不同，但两个频繁类之间和两个少数类之间的数据量相同。
             两者都是自然界中普遍存在的数据不平衡现象。

2020.11.15（周日）晚19：00
张：论文：Does William Shakespeare REALLY Write Hamlet?Knowledge Representation
       Learning with Confidence
       Q1:关于论文中作者提出的路径是如何得到的？
              作者通过枚举的方式得到的路径集合
       Q2:Relation Path Reliability是如何体现可靠地？
              作者采用资源分配的思想，给一个head分配固定数量的资源，假设为1，其中e1∈Eii1(·,e)表示关系r对应的前驱实体，|Ei(e1, ·)|表示关系r前驱
              实体中相同关系的后继节点数的倒数，Rp(e)表示实体分配的资源，即一个节点获得的资源由前驱节点分配给他的资源总和，而前驱节点分配的资源
              由Rp(e1)/|Ei(e1, ·)|得到；例如求节点m的分配资源，n为它的一个前驱，三元组（n,r,m），n在相同关系下的尾实体可能是多个记为x，则|Ei(e1, ·)|
              =x；通过这样如果这条路径分配的资源越多，说明这条关系路径越可靠；
鲍：论文：Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

2020.11.8（周日）晚19:00
傅：论文：application of autoencoder in novelty detection：Latent Space Autoregression for Novelty Detection
             ROBUST SUBSPACE RECOVERY LAYER FOR UNSUPERVISED ANOMALY DETECTION
             NOVELTY DETECTION WITH RECONSTRUCTION ALONG PROJECTION PATHWAY
魏：论文：Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations

2020.11.1（周日）晚19：00
倪：论文：Learning from Noisy Labels with Deep Neural Networks: A Survey
       Q1：Roubust function具体应该怎么写？
           以GCE为例，它综合了MAC与CCE的优点
           令于是它在q->0时趋向于CCE，在q->1时趋向于MAE

       Q2：用于预测噪声类型和预测噪声的模块具体是怎么工作的？
           用有干净标签的数据与它的噪声标签放入神经网络训练，训练出两个模型。
           一个预测数据是无噪声(noise free)以及有噪声的噪声种类(random,cofusing)；另一个模型预测它的标签。两者综合得到它的最终预计标签。

       Q3：Loss correction中的label transition matrix T是如何定的？
           T如果有的话可以直接给模型用来训练；没有的时候需要先训练出一个T
傅：论文：application of autoencoder in novelty detection：Latent Space Autoregression for Novelty Detection
             ROBUST SUBSPACE RECOVERY LAYER FOR UNSUPERVISED ANOMALY DETECTION
             NOVELTY DETECTION WITH RECONSTRUCTION ALONG PROJECTION PATHWAY

2020.10.24（周日）晚19：00
张：GCN论文：SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS
      Q1:在GCN论文中有关作者的实验结果中GCN (rand. splits)的疑问？
            作者在这里交叉验证他们的模型在10个随机绘制的相同大小的数据集分割上的性能。
魏：论文：Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
郭：论文：Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey
      Q1:Random noise和Y-dependent random noise有什么区别?
        Y-dependent random noise变化到其他类的概率是随机的，而变到自身true class的概率不随机，没有找到相关资料，我推断是先给定这个概率，然后变到其他类的概率再随机，因此这
