每周三晚19:00 周期会议ID：385-8349-2666
每周六晚19:00(周报) 会议ID：897-7427-3526

2022.12.7（周三）晚上19:00
海亮：AFEC: Active Forgetting of Negative Transfer in Continual Learning
     Q1：在持续学习过程中，模型的大小有无变化？
          没有变化，在模型的突触扩展阶段，会初始化一个和原模型一样大小的新模型去学习新任务；然后在突触汇聚阶段，在原模型上进行学习新任务，同时利用突触扩展阶段学习的新模型来指导原模型，最后在推理阶段只使用原模型进行推理。
     Q2：遗忘因子β最后在公式中体现在哪？
          最后体现在λ_e这个超参数中，在优化的推理过程中我们可以得到新添加的正则化项前面的系数应该正比于β/(1-β)，因此只需要寻找最优的超参数λ_e就可以控制对旧知识的主动遗忘。
     Q3：在损失函数中，新任务上的交叉熵损失和新添加的损失的区别？
          新任务上的交叉熵损失是模型在学习完A任务的基础上进行的学习，这其中包含了模型负迁移的影响，而且损失中的第二项会让模型保留旧知识，这会加剧负迁移的影响。而新添加的损失是任务B在随机初始化任务上训练模型的指导，可以有效缓解负迁移的现象。
若璇：Balanced Multimodal Learning via On-the-fly Gradient Modulation
     Q1：根据最近的研究，在联合训练策略下优化所有模式的统一学习目标的多模态模型在某些情况下可能不如单模态模型。这个问题最初是由《What makes training multi-modal classification networks hard?》这篇论文提出的，这篇文章有做方法和效果上的对比吗？有什么联系？
          《What makes training multi-modal classification networks hard?》提出了多模态训练时不同模态的模型具有不同的收敛速度，这启发了本文。两者使用的具体方法不同。具体的方法比较可以参考原文。
     Q2：使用了本文的OGM-GE策略后，在单个模态上的效果仍然没有超过单模态模型，什么原因？能否进一步改善？
          原因值得进一步探索。
     Q3：强弱模态的具体表现是什么，能否从数学上进行原理解释？
          本文有从数学原理上进行一定的解释。在梯度反向传播的过程中，A模态的计算会影响到B模态的梯度计算，导致另一模态的训练优化不完全，抑制其优化。但是如何抑制的值得进一步研究。
志浩：CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks
     Q1：上游多模态任务是如何学习的？
          根据多模态任务的次序，依次学习，并且验证模型对未来任务的迁移能力以及对过去任务的遗忘程度。
     Q2：任务的Backbone选用的是哪一个模型？
          选取的是单流的预训练ViLT模型，通过在模型的Transformer Encoder上添加特定的分类头对多模态任务进行微调。

2022.11.30（周三）晚上19:00
晁典：Not All Labels Are Equal:Rationalizing The Labeling Costs for Training Object Detection
     Q1：模型是半监督还是全监督，主要思路是什么？
          半监督，利用主动学习挑选半监督当中不一致性高的未标记样本进行人工标记，提升模型性能。
     Q2：为什么能够挑选out of distribution的样本？
          这篇论文考虑了两种类型的out of distribution的样本，一种是置信度高但是与增强版本不一致性高的图片，还有一种是置信度低因此熵值高的图片，综合计算评分，评分高的大概率是类里面没有的样本。
     Q3：为什么使用半监督不一致方法会损害模型？
          因为半监督中不一致性方法假定图片增强前后趋于一致，但是这样做对于一些特殊的标记错的样本会出现预测错的情况，损害模型。

2022.11.23（周三）晚上19:00
文涓：ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection
初兵：Comprehensive Fair Meta-learned Recommender System

2022.11.16（周三）晚上19:00
士枫：Generating Perturbation-based Explanations with Robustness to Out-of-Distribution Data
禹睿：Are Multimodal Transformers Robust to Missing Modality?

2022.11.9（周三）晚上19:00
舟阳：竞赛相关
宏鹏：Class-Imbalanced Semi-Supervised Learning with Adaptive Thresholding

2022.11.2（周三）晚上19:00
高奕：RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning
祥宇：Grounded Language-Image Pre-training
志浩：FILIP FINE-GRAINED INTERACTIVE LANGUAGE IMAGE PRE-TRAINING

2022.10.26（周三）晚上19:00
守威：OpenMatch Open-set Consistency Regularization for Semi-supervised Learning with Outliers
文涓：Denoising Diffusion Probabilistic Models

2022.10.19（周三）下午14:30
伯元：Fine-grained Angular Contrastive Learning with Coarse Labels

2022.10.12（周三）晚上19:00
禹睿：OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION
林海：GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features

2022.10.5（周三）下午14:30
初兵：Towards Universal Sequence Representation Learning for Recommender Systems
海亮：Meta-attention for ViT-backed Continual Learning

2022.9.30（周五）晚上19:00
志浩：Hierarchical Text-Conditional Image Generation with CLIP Latents
中添：MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space
舟阳：Expanding Language-Image Pretrained Models for General Video Recognition

2022.9.24（周六）晚上19:00
祥宇：Distilled Dual-Encoder Model for Vision-Language Understanding
宇萱：HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions

2022.9.17（周六）晚上19:00
红陈：PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR OBJECT DETECTION
文涓：Fake It Till You Make It: Near-Distribution Novelty Detection by Score-Based Generative Models

2022.9.9（周五）下午14:30
禹睿：The Model May Fit You: User-Generalized Cross-Modal Retrieval
金一：CDTRANS: CROSS-DOMAIN TRANSFORMER FOR UNSUPERVISED DOMAIN ADAPTATION
